\documentclass[runningheads]{llncs}
\usepackage{xcolor}
\usepackage{url}
\usepackage{graphicx} 
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usetikzlibrary{arrows.meta, positioning, fit}

\definecolor{uf_blue}{RGB}{17,27, 150}
\definecolor{uf_orange}{RGB}{150,100,17}

\begin{document}

\title{Learning to Dispatch: A Reinforcement Learning Framework for Train Dispatch Networks}
\titlerunning{\textcolor{uf_blue}{Learning to Dispatch}}

\author{\textcolor{uf_blue}{Andres Espinosa}}
\institute{
    \textcolor{uf_orange}{Industrial and Systems Engineering \\
    University of Florida}\\
    \textcolor{uf_orange}{\email{andresespinosa@ufl.edu}}
}
\maketitle


\begin{abstract}
    Train dispatching is a complex and critical task in Operations Research that has traditionally depended on human expertise or static rule-based systems, both of which can lead to inefficiencies and errors. 
    The Train Dispatching Problem (TDP) focuses on optimizing train movements to reduce delays while obeying safety and physical scheduling constraints.
    Reinforcement Learning (RL), particularly Deep Reinforcement Learning (DRL) combined with Graph Neural Networks (GNNs), offers a promising new approach for tackling this challenge in real time. 
    This paper presents a framework that models rail networks as graph structures, using GNNs to process these configurations within an RL framework. 
    This approach integrates Train Operation Graphs, which model individual train activities, with Resource Conflict Graphs to construct a comprehensive problem graph representation. 
    A graph-based Deep Q-Network (DQN) agent is used to evaluate the formulation and shows potential for improving dispatching efficiency by learning effective policies. 
    Expanding on this thesis, further work is needed to refine the action space, address deadlocks, and explore hybrid methods with Mixed-Integer Programming (MIP) solvers. 
    This study lays the groundwork for transforming the train dispatching problem into a reinforcement learning model for further advancements in RL applications to the TDP.
\end{abstract}



\section{Introduction}
\label{sse:introduction}

% What is the problem?
The Train Dispatching Problem (TDP), widely recognized as a classic and challenging problem in Operations Research (OR), concerns the real-time management of train movements across a rail network.
This involves deciding when and where trains should move, stop, or wait, based on factors such as schedules, track availability, and priority rules. 
Dispatching decisions must be made continuously and quickly, especially in high-traffic networks, making the problem both operationally critical and computationally challenging.

% Why does the problem exist?
Despite significant technological advances in other areas of transportation and logistics, train dispatching remains heavily reliant on human decision-makers or static rule-based systems. 
This is largely because the problem is highly combinatorial: even with the smallest problems, there can be exponentially many possible sequences of decisions. 
Human dispatchers bring experience and intuition to these situations, but they are limited in how much information they can process and how consistently they can manage large-scale disruptions or optimize traffic flow over time.

% Why solve this problem?
Improving train dispatching systems has the potential to reduce passenger delays, minimize dispatching errors, and prevent deadlocks—where no train can proceed without violating physical, safety, or scheduling constraints. 
By optimizing dispatching we can also improve energy efficiency and capacity utilization, making rail transport more sustainable. 
In dense urban transit systems or busy freight corridors, even marginal improvements in dispatching can lead to significant gains in resource consumption or time savings.

% Why solve with RL?
Reinforcement Learning (RL) offers a new approach to tackling the TDP by framing it as a sequential decision-making problem.
By formulating the TDP as an RL problem, an agent learns to make dispatching decisions through interactions with a simulated environment, similar to how people learn through trial and error. 
RL is an active area of research particularly well-suited for problems with delayed consequences, dynamic environments, and large state spaces—all of which apply to train dispatching. 

% Project scope.
This work focuses on the formulation of the Train Dispatching Problem for RL-based approaches, rather than on the algorithm implementation. 
This thesis emphasizes how the problem can be encoded as an RL task using graph structures to represent rail networks and their many constraints. 
The goal is to provide a robust and extensible framework that future researchers and practitioners can build upon when applying RL methods to train dispatching and related infrastructure scheduling problems.
As demonstrated in recent research on applying RL to car and pedestrian-level traffic, combing RL and graph structures can yield powerful results, so long as the algorithms and models are capable of interpreting the underlying graphs \cite{gnndrl:Devailly_2022}.

% Paper Outline
To provide a thorough understanding of the approach, the remainder of this paper is organized as follows. 
In Section~\ref{sse:background}, we present the foundational background necessary for our formulation, beginning with a formal description of the Train Dispatch Problem, Reinforcement Learning, and Graph Neural Networks.
These concepts are then reinforced in a discussion of recent research that inspired this work.
Section~\ref{sse:formulation} details the proposed formulation, introducing Train Operation Graphs (\ref{sss:train_ops}), Resource Conflict Graphs (\ref{sss:resource_conflicts}), and the state and action spaces (\ref{sss:state_space}, \ref{sss:action_space}). 
Section~\ref{sse:results} presents the results of implementing a preliminary graph-based Deep Q-Network (DQN) agent for the Train Dispatching Problem. 
It highlights the design of the graph-based DQN agent, the reward function employed, and the limitations observed during experimentation.
Finally, Section~\ref{sse:conclusion} concludes this thesis and highlights a few future directions for work.



\section{Problem Background}
\label{sse:background}
\subsection{Train Dispatch Problem}
\label{sss:train}
% This should cover the MIP formulation and the general outline of the problem
% This should also cover the DISPLIB data format and all that.
% Reinforcement Learning (RL) has demonstrated significant potential in solving complex, real-world problems, achieving notable successes in domains such as games \cite{drl:rl_games_industry}, robotics \cite{drl:robotics_review}, and supply chain optimization \cite{drl:rl_supply_chain}.
The TDP is a well-known example of a large-scale combinatorial optimization problem, characterized by its complexity and the need for efficient decision-making under constraints. 
This problem becomes especially critical in dense networks with high traffic volume, where suboptimal dispatching can lead to cascading delays, congestion, or even deadlocks.
Traditional approaches typically formulate TDP as a Mixed-Integer Programming (MIP) problem, where binary decision variables represent route allocations, resource occupations (e.g., track segments), and train movements over time \cite{train:doi:10.1287/opre.2014.1327}. 
Constraints in this formulation encode physical infrastructure limitations, train precedence relations, safety buffers, and scheduling windows \cite{train:LAMORGESE2013559}. 
While MIP solvers can produce high-quality solutions, they are often limited by scalability, especially under tight time constraints or when dealing with real-time disruptions \cite{train:Sch_licke_2025}.
Generally, the TDP presents a significant challenge for existing algorithms due to: \begin{itemize} \item Its high-dimensional decision space and tight operational constraints \item The need for fast and adaptive solutions under uncertainty \item The opportunity to drastically improve throughput, punctuality, and network resilience. \end{itemize}

In practice, the TDP is currently managed by human dispatchers using rule-based decision support systems to assist the scheduling process. 
Human expertise plays a crucial role in adapting to disruptions or prioritizing certain services, but manual dispatching lacks scalability and consistency. 
Rule-based systems, while automated, are quite inflexible and often suboptimal when conditions change. 
These limitations open the door for machine learning-based methods, particularly those capable of learning scalable policies from data and interaction.

The DISPLIB 2025 Train Dispatching Competition, which inspires this work, provides a standardized and open-source benchmark dataset for evaluating train dispatching algorithms \cite{train:DISPLIB2025}. 
DISPLIB scenarios model complex networks and real-world operational constraints, enabling rigorous benchmarking. 
Each instance in DISPLIB specifies a railway topology, time constraint information, and resource usage requirements. 
The formulation presented in Section~\ref{sse:formulation} is deeply integrated with this standardized dataset.

Given the current lack of RL-based solutions and the standardized dataset provided by the DISPLIB competition, the TDP stands as a compelling application domain for RL and Graph-based learning models.

\subsection{Deep Reinforcement Learning}
\label{sss:reinforcement_learning}
% Cover Markov Decision Processes
% As well as DQN
Deep Reinforcement Learning (DRL) is a subfield of machine learning that combines RL with deep learning techniques to solve complex decision-making problems. 
RL focuses on training agents to make sequential decisions by interacting with an environment, receiving feedback in the form of rewards, and learning to maximize cumulative rewards over time. 
Deep learning, on the other hand, enables the representation of high-dimensional data through neural networks, making it possible to handle complex state and action spaces.

The seminal work by Mnih et al.~\cite{drl:Mnih13} introduced the Deep Q-Network (DQN), which demonstrated the ability of DRL to learn policies directly from raw pixel inputs in Atari games. 
This approach used a convolutional neural network to approximate the Q-value function, enabling the agent to make decisions in high-dimensional state spaces. 
The success of DQN highlighted the potential of combining deep learning with RL to tackle problems that were previously intractable.

These advancements have established DRL as a versatile and powerful tool for solving sequential decision-making problems in diverse domains, including robotics, healthcare, and transportation systems.
\subsubsection{Markov Decision Processes}
Markov Decision Processes (MDPs) form the backbone of reinforcement learning. 
MDPs offer a structured framework for modeling decision-making in environments where outcomes are influenced by both randomness and the agent's actions. 
An MDP is formally defined by the tuple $(S, A, P, R, \gamma)$, where:

\begin{itemize}
    \item $S$: A finite set of states representing all possible configurations of the environment.
    \item $A$: A finite set of actions available to the agent.
    \item $P(s'|s, a)$: The state transition probability function, which defines the probability of transitioning to state $s'$ given the current state $s$ and action $a$.
    \item $R(s, a)$: The reward function, which specifies the immediate reward received after taking action $a$ in state $s$.
    \item $\gamma \in [0, 1]$: The discount factor, which determines the importance of future rewards relative to immediate rewards.
\end{itemize}

In essence, an MDP is a process where the current state contains all the information necessary to determine the optimal action and its associated reward. 
In RL, this process is typically modeled as an interaction between an agent and an environment, governed by underlying dynamics. 
At each timestep, the environment provides the current state $s_t$ to the agent, which selects an action $a_t$. 
This action is then applied to the environment, which updates its state to $s_{t+1}$ and computes the corresponding reward $r_{t+1}$. 
This iterative process is illustrated in Figure~\ref{fig:rl_framework}.

\begin{figure}
    \centering
    \begin{tikzpicture}[
      agent/.style={draw, rounded corners, fill=blue!20, minimum width=2.5cm, minimum height=1.2cm},
      env/.style={draw, rounded corners, fill=green!20, minimum width=2.5cm, minimum height=1.2cm},
      reward/.style={draw, rounded corners, fill=orange!30, minimum width=2.5cm, minimum height=1.2cm},
      arrow/.style={->, thick, >=Stealth},
      every node/.style={font=\small}
      ]
    
    % Nodes
    \node[agent] (agent) at (0, 2) {Agent};
    \node[env] (env) at (-2, -2) {Environment};
    \node[reward] (reward) at (2, -2) {Dynamics};
    
    % Arrows
    \draw[arrow] (agent) to[bend right=25] node[above left] {$a_t$} (env);
    \draw[arrow] (env) to[bend right=25] node[below] {$s_{t+1}$} (reward);
    \draw[arrow] (reward) to[bend right=25] node[above right] {$s_t$} (agent);
    \draw[arrow] (env) to[bend left=25] node[below right] {$r_{t+1}$} (reward);
    
    \end{tikzpicture}
    \caption{The simplified markov decision process and reinforcement learning framework.}
    \label{fig:rl_framework}
\end{figure}

The goal in an MDP is to find a policy $\pi(a|s)$, which is a mapping from states to actions, that maximizes the expected cumulative reward, also known as the return:
\[
G_t = \mathbb{E} \left[ \sum_{k=0}^\infty \gamma^k R(s_{t+k}, a_{t+k}) \right].
\]

MDPs are widely used in RL to model environments where agents learn optimal policies through interaction and feedback.

\subsubsection{Deep Q-Network}
Deep Q-Networks (DQN) are a class of RL algorithms that approximate the Q-value function using deep neural networks. 
The Q-value function, $Q(s, a)$, represents the expected cumulative reward of taking action $a$ in state $s$ and following the optimal policy thereafter. 
The DQN agent learns this function through experience replay and iterative updates.

The key components of a DQN agent are:

\begin{itemize}
    \item \textbf{Q-Network:} A neural network that takes the current state $s_t$ as input and outputs Q-values for all possible actions. 
    \item \textbf{Target Network:} A separate network with weights $\theta^-$, used to stabilize training by providing target Q-values. The target network is periodically updated to match the Q-network.
    \item \textbf{Experience Replay:} A buffer that stores past experiences $(s_t, a_t, r_t, s_{t+1})$. During training, mini-batches of experiences are sampled randomly from the buffer to break correlations and improve learning stability.
    \item \textbf{Bellman Equation:} The Q-network is trained to minimize the temporal difference (TD) error, defined as:
    \[
    \delta = \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-) - Q(s_t, a_t; \theta) \right],
    \]
    where $\gamma$ is the discount factor.
    \item \textbf{Exploration vs. Exploitation:} The agent balances exploration (choosing new actions to explore the environment) and exploitation (choosing actions to exploit what it already knows).
\end{itemize}

The training process involves iteratively updating the Q-network's weights $\theta$ using gradient descent to minimize the Temporal Difference (TD) error. 
The learned Q-values guide the agent in selecting actions that maximize long-term rewards.
For simpler tasks, a basic Q-network may suffice (such as a linear regression or heuristic model). 
However, for complex problems like the TDP, more advanced neural network architectures are likely better suited to handle the challenges effectively.


\subsection{Graph Neural Networks}
\label{sss:gnn}
% Cover graph neural networks
Graph Neural Networks (GNNs) are a class of neural networks designed to operate on graph-structured data, where entities are represented as nodes and their relationships as edges. 
GNNs leverage the graph topology and node features to learn meaningful representations for tasks such as node classification, link prediction, and graph classification.

GNNs extend traditional neural networks by incorporating message-passing mechanisms, where nodes aggregate information from their neighbors iteratively. 
This process enables the network to capture both local and global graph structures. 
The key components of GNNs include:

\begin{itemize}
    \item \textbf{Message Passing:} Nodes exchange information with neighbors through a series of message-passing steps. At each step, a node updates its representation by aggregating messages from its neighbors.
    \item \textbf{Aggregation Functions:} Common aggregation functions include summation, mean, and max-pooling.
    \item \textbf{Update Functions:} After aggregation, node representations are updated using learnable functions, often implemented as neural networks.
    \item \textbf{Pooling Layers:} For graph-level tasks, pooling layers summarize the entire graph into a fixed-size representation.
\end{itemize}

GNNs can be thought of as the graph-based counterpart to Convolutional Neural Networks (CNNs) for images. 
GNNs effectively aggregate information from neighboring nodes and edges, enabling each node to build a representation that incorporates both its own features and the context provided by its local graph structure.
They then apply a pooling operation to extract and summarize key patterns and trends from the data.
This comparison is illustrated in Figure~\ref{fig:cnn_vs_gnn}

\begin{figure}[htbp]
    \centering
    % CNN
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[
            image/.style={draw, rounded corners, fill=orange!30, minimum width=2cm, minimum height=2cm},
            filter/.style={draw, thick, fill=green!20, minimum width=1cm, minimum height=1cm, rounded corners},
            feature/.style={draw, thick, fill=green!20, minimum width=1cm, minimum height=0.6cm, rounded corners},
            ann/.style={draw, thick, fill=purple!30, circle, minimum size=0.8cm},
            arrow/.style={->, thick, >=Stealth},
            every node/.style={font=\scriptsize}
        ]

        % Nodes
        \node[image] (input) at (0, 0.25) {\includegraphics[width=1.5cm]{example-image}};
        \node[filter] (filter) at (0, -1.75) {Filter};
        \node[feature] (feature) at (0, -3) {Pooling};
        \node[ann] (ann) at (0, -4.5) {NN};

        % Arrows
        \draw[arrow] (input) -- (filter);
        \draw[arrow] (filter) -- (feature);
        \draw[arrow] (feature) -- (ann);

        \end{tikzpicture}
        \caption{Convolutional Neural Network}
    \end{subfigure}
    \hfill
    % GNN
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[
            node/.style={circle, draw, fill=orange!30, minimum size=0.8cm},
            message/.style={draw, thick, fill=blue!20, minimum width=1cm, minimum height=0.6cm, rounded corners},
            aggregate/.style={draw, thick, fill=green!20, minimum width=1cm, minimum height=0.6cm, rounded corners},
            arrow/.style={->, thick, >=Stealth},
            doublearrow/.style={<->, dotted, thick, >=Stealth},
            every node/.style={font=\scriptsize}
        ]

        % Nodes
        \node[node] (A) at (0, 0) {A};
        \node[node] (B) at (1.5, 1) {B};
        \node[node] (C) at (1.5, -1) {C};
        \node[node] (D) at (3, 0) {D};

        % Edges
        \draw[doublearrow] (A) -- (B);
        \draw[doublearrow] (A) -- (C);
        \draw[doublearrow] (B) -- (D);
        \draw[doublearrow] (C) -- (D);

        % Message Passing
        \node[message] (msg) at (1.5, -2) {Message Passing};
        \draw[arrow] (C) -- (msg);
        \draw[arrow] (A) -- (msg);
        \draw[arrow] (D) -- (msg);

        % Aggregation
        \node[aggregate] (agg) at (1.5, -3) {Aggregation};
        \draw[arrow] (msg) -- (agg);

        % Pooling
        \node[aggregate] (pooling) at (1.5, -4) {Pooling};
        \draw[arrow] (agg) -- (pooling);

        % Output
        \node[node, fill=purple!30, minimum size=0.8cm, circle] (output) at (1.5, -5) {NN};
        \draw[arrow] (pooling) -- (output);

        \end{tikzpicture}
        \caption{Graph Neural Network}
    \end{subfigure}
    \caption{Comparison of CNN and GNN architectures.}
    \label{fig:cnn_vs_gnn}
\end{figure}


\subsubsection{Applications in Reinforcement Learning}
GNNs have shown significant promise in RL for tasks involving graph-structured environments. 
Munikoti et al.~\cite{gnndrl:munikoti2022challengesopportunitiesdeepreinforcement} provide a comprehensive review of how GNNs can enhance RL by enabling agents to reason over relational data, such as traffic networks, infection models, or social graphs. 
They highlight the ability of GNNs to generalize across varying graph sizes and structures.

GNNs have been successfully integrated with RL to address complex graph-structured problems across various domains. 
For example, Almasan et al.~\cite{gnndrl:ALMASAN2022184} applied GNNs to network routing, demonstrating their ability to optimize routing decisions in dynamic environments.
Tackling the problem of influence maximization in social networks, GraMeR~\cite{gnndrl:munikoti2022gramergraphmetareinforcement} leverage GNNs combined with meta-reinforcement learning to learn transferable policies across diverse graph instances.

Devailly et al.~\cite{gnndrl:Devailly_2022} introduced IG-RL, an inductive graph reinforcement learning approach for large-scale traffic signal control which enables scalable and adaptive control strategies. 
Similarly to this thesis, Agasucci et al.~\cite{gnndrl:agasucci} introduced a DRL-based approach to the train dispatching problem, though their method is limited by the specificity of the dataset employed.

In summary, GNNs offer a robust framework for learning on graph-structured data, significantly augmenting the set of possible reinforcement learning applications.
The ability of GNNs to learn from dynamic graph-structured data is a core concept behind this formulation.

\section{Formulation}
\label{sse:formulation}
The following formulation introduces an approach to the TDP leveraging both graph-based representations and reinforcement learning techniques. 
By modeling train operations as Directed Acyclic Graphs (DAGs) and resource conflicts as Resource Conflict Graphs (RCGs), this formulation aims to capture the dependencies and constraints inherent to railway systems. 
The integration of GNNs is a crucial component that enables the reinforcement learning agent to process these graph structures effectively.

The remainder of this section is organized as follows. 
Subsection~\ref{sss:train_ops} introduces the concept of Train Operation Graphs in the DISPLIB format, detailing their structure and role in representing train operations and constraints. 
Subsection~\ref{sss:resource_conflicts} describes the Resource Conflict Graphs, which model resource-sharing conflicts between trains. Subsection~\ref{sss:state_space} defines the state space, highlighting how graph-based representations encode the environment's current state. 
Finally, Subsection~\ref{sss:action_space} outlines the action space, specifying the set of actions available to the reinforcement learning agent for decision-making.

\subsection{Train Operation Graphs}
\label{sss:train_ops}

The Train Operation DAG format provides a structured representation of train operations and their constraints. 
Conceptually, each train is modeled as a DAG, called the operations graph, where each node represents a possible operation the train can choose and each edge represents the transitions between operation.
The primary components of the operations graphs are outlined as follows:

\begin{itemize}
    \item \textbf{Nodes:} Represent operations, each with:
    \begin{itemize}
        \item A \textbf{minimum duration} $\delta$, indicating the shortest permissible time for the operation.
        \item \textbf{Start time bounds} $[t_{\text{min}}, t_{\text{max}}]$, specifying the allowable time window for the operation to commence.
        \item A set of \textbf{resources} required exclusively during the operation, each potentially associated with a \textbf{release time}. These resources typically represent the rail track section occupied by the operation.
    \end{itemize}
    \item \textbf{Edges:} Define precedence constraints between operations, ensuring that dependent operations are executed in the correct order.
    \item \textbf{Entry and Exit Nodes:} The graph has a single \textbf{entry operation} (no incoming edges) and a single \textbf{exit operation} (no outgoing edges).
\end{itemize}

A visual example of a very simple train's operation graph can be seen in Figure~\ref{fig:train_dag}.
In this small example, the train has only one decision to make: choosing between operation 1 and operation 2. 
It then proceeds to operation 3, which has a minimum duration of 5, before moving to the exit operation.


\begin{figure}
    \centering
    \begin{tikzpicture}[
        node/.style={circle, draw, fill=blue!20, minimum size=1.2cm, align=center},
        edge/.style={->, thick},
        resource/.style={draw, rounded corners, fill=orange!30, minimum width=1.5cm, minimum height=1cm},
        every node/.style={font=\small}
    ]
        % Nodes
        \node[node] (op0) at (0, 0) {op 0 \\ $\delta = 5$ \\ $[]$};
        \node[node] (op1) at (3, 1.5) {op 1 \\ $\delta = 0$ \\ $r_2$};
        \node[node] (op2) at (3, -1.5) {op 2 \\ $\delta = 0$ \\ $r_1, r_2$};
        \node[node] (op3) at (6, 0) {op 3 \\ $\delta = 5$ \\ $r_1$};
        \node[node] (op4) at (9, 0) {op 4 \\ $\delta = 0$ \\ $[]$};

        % Edges
        \draw[edge] (op0) -- (op1);
        \draw[edge] (op0) -- (op2);
        \draw[edge] (op1) -- (op3);
        \draw[edge] (op2) -- (op3);
        \draw[edge] (op3) -- (op4);

        % Additional DAG
        % \node[node] (op0_new) at (0, -4) {op 0 \\ $\delta = 0$ \\ $[]$};
        % \node[node] (op1_new) at (3, -4) {op 1 \\ $\delta = 0$ \\ $r_1$};
        % \node[node] (op2_new) at (6, -4) {op 2 \\ $\delta = 0$ \\ $[]$};

        % % Edges
        % \draw[edge] (op0_new) -- (op1_new);
        % \draw[edge] (op1_new) -- (op2_new);

        % \draw[<->, red, thick] (op1) -- (op2);
        % \draw[<->, red, thick] (op3) -- (op2);
        % \draw[<->, red, thick] (op1_new) -- (op2);
        % \draw[<->, red, thick] (op1_new) -- (op3);

    \end{tikzpicture}
    \caption{Example Train Operation DAG with resource and duration constraints.}
    \label{fig:train_dag}
\end{figure}

\subsubsection{Objective Function}
The DISPLIB competition defines the objective as minimizing delay costs, calculated as:
\[
v_i = c \cdot \max(0, t - t_{\text{threshold}}) + d \cdot H(t - t_{\text{threshold}})
\]
where $c$ and $d$ are constants, $t$ is the operation's start time, and $H$ is the Heaviside step function \cite{train:DISPLIB2025}.
Alternative objective functions can also be considered, such as minimizing the total time required for all trains to complete their respective exit operations.

The Train Operation Graphs serve as the foundation for modeling the sequential decision-making process for one train.
However, these graphs alone are insufficient to capture the interactions and conflicts between multiple trains sharing the same resources. 


\subsubsection{Feasibility Conditions}
A solution is feasible if:
\begin{enumerate}
    \item Events are ordered chronologically.
    \item Each train's operations form a valid path from the entry to the exit operation.
    \item Start times respect the bounds $[t_{\text{min}}, t_{\text{max}}]$.
    \item Operation durations meet or exceed the minimum $\delta$.
    \item Resource constraints are satisfied across all trains.
\end{enumerate}


\subsubsection{Resources and Constraints}
Resources represent entities (e.g., track sections) that cannot be shared between trains simultaneously. 
Operations requiring the same resource must satisfy the following:
\begin{itemize}
    \item The \textbf{end event} of one operation must precede the \textbf{start event} of the next.
    \item The \textbf{release time} of the resource must be respected.
\end{itemize}
To abstract resource labels from individual operations, this formulation introduces a new type of edge to represent resource conflicts between operations. 
\subsection{Resource Conflict Graphs}
\label{sss:resource_conflicts}
A Resource Conflict Graph abstracts resources as edges, rather than treating resources as attributes of individual operations.
These edges connect operations that share the same resource, transforming the disjointed train operation DAGs into a unified graph structure. 

Figure~\ref{fig:train_rcg} illustrates this concept, expanding on the toy example in Figure~\ref{fig:train_dag} with two trains. 
In this example, operation 2 of Train A utilizes resource $r_1$, which is also required by operation 3 of Train A and operation 1 of Train B. 
By introducing resource conflict edges, the formulation captures these dependencies and creates a "super graph" that integrates all train operation DAGs into a single, connected graph.

This unified graph serves as the foundation for the state space in the Markov Decision Process (MDP), encapsulating both individual train operations and their resource-sharing constraints. 
Such a representation enables the reinforcement learning agent to reason about both local and global interactions within the railway network.

\begin{figure}
    \centering
    \begin{tikzpicture}[
        node/.style={circle, draw, fill=blue!20, minimum size=1.2cm, align=center},
        edge/.style={->, thick},
        resource/.style={draw, rounded corners, fill=orange!30, minimum width=1.5cm, minimum height=1cm},
        every node/.style={font=\small}
    ]
        % Nodes
        \node[node] (op0) at (0, 0) {op 0 \\ $\delta = 5$ \\ $[]$};
        \node[node] (op1) at (3, 1.5) {op 1 \\ $\delta = 0$ \\ $r_2$};
        \node[node] (op2) at (3, -1.5) {op 2 \\ $\delta = 0$ \\ $r_1, r_2$};
        \node[node] (op3) at (6, 0) {op 3 \\ $\delta = 5$ \\ $r_1$};
        \node[node] (op4) at (9, 0) {op 4 \\ $\delta = 0$ \\ $[]$};

        % Edges
        \draw[edge] (op0) -- (op1);
        \draw[edge] (op0) -- (op2);
        \draw[edge] (op1) -- (op3);
        \draw[edge] (op2) -- (op3);
        \draw[edge] (op3) -- (op4);

        % Additional DAG
        \node[node] (op0_new) at (0, -4) {op 0 \\ $\delta = 0$ \\ $[]$};
        \node[node] (op1_new) at (3, -4) {op 1 \\ $\delta = 0$ \\ $r_1$};
        \node[node] (op2_new) at (6, -4) {op 2 \\ $\delta = 0$ \\ $[]$};

        % Edges
        \draw[edge] (op0_new) -- (op1_new);
        \draw[edge] (op1_new) -- (op2_new);

        \draw[<->, red, thick] (op1) -- (op2);
        \draw[<->, red, thick] (op3) -- (op2);
        \draw[<->, red, thick] (op1_new) -- (op2);
        \draw[<->, red, thick] (op1_new) -- (op3);

    \end{tikzpicture}
    \caption{Example "super graph" with two trains, Train A and Train B. These individual operation graphs are connected by their resource conflicts, signified by red edges.}
    \label{fig:train_rcg}
\end{figure}

\subsection{State Space}
\label{sss:state_space}

In this formulation, the state space is represented as a "super graph" composed of operation graphs connected by their resource conflict edges. 
Each node in the graph corresponds to an operation and is characterized by a feature vector that captures relevant attributes of the operation. 
The node feature vector includes the following parameters:

\begin{itemize}
    \item $x_1$: Train ID of the operation.
    \item $x_2$: Operation ID of the operation.
    \item $x_3$: Start lower bound of the operation (0 if not present).
    \item $x_4$: Start upper bound of the operation ($10^6$ if not present).
    \item $x_5$: Minimum duration of the operation (0 if not present).
    \item $x_6$: Can pick (1 if the operation has not been picked and can be picked, 0 otherwise).
    \item $x_7$: Currently picked (1 if a train is currently handling this operation, 0 otherwise).
    \item $x_8$: Picked (1 if the operation has already been picked, 0 otherwise).
    \item $x_9$: Not picked yet (1 if the operation has not been picked but can be picked in the future, 0 otherwise).
    \item $x_{10}$: Feasible (1 if the operation can be picked and does not violate any constraints, 0 otherwise).
    \item $x_{11}$: Cannot be picked (1 if the operation has not been picked and cannot be picked in the future, 0 otherwise).
    \item $x_{12}$: Threshold of the operation (0 if not present).
    \item $x_{13}$: Haversine component of the operation (0 if not present).
    \item $x_{14}$: Linear delay of the operation (0 if not present).
    \item $x_{15}$: Operation start time (0 if not started yet).
    \item $x_{16}$: Operation end time (0 if not ended yet).
\end{itemize}

Edges in the graph are assigned a feature vector that captures the type of relationship and resource constraints. 
The edge feature vector is defined as:
\begin{itemize}
    \item $e_1$: Operation type (1 if this edge is between two train operations, 0 o/w)
    \item $e_2$: Resource type (1 if this edge represents a resource share, 0 o/w)
    \item $e_3$: Release time (the release time of the incoming operation's resource, 0 if there is no release time or if it is an operation type edge)
\end{itemize}

In addition to the node and edge features, the graph is also characterized by a global feature vector, denoted as \textbf{y}. 
This graph-level feature vector contains a single entry, representing the current timestep of the solution. 
By including this global feature, the model gains a crucial awareness of the temporal context.

This representation provides a simple yet comprehensive structure for encoding the operations and their relationships, which can be processed by a GNN to generate a graph embedding for downstream decision-making tasks.

\subsection{Action Space}
\label{sss:action_space}
The action space in this formulation consists of three primary actions that the agent can take at each decision step:

\begin{itemize}
    \item \textbf{Add Node:} The agent selects a specific operation node from the graph to be added to the current schedule. This action is only valid if the selected node satisfies all feasibility conditions, such as respecting precedence constraints and resource availability. The agent must choose one node from the set of feasible nodes.

    \item \textbf{Discard Node:} The agent chooses to discard a specific operation node, indicating that the operation will not be scheduled. This action is irreversible and is typically used for operations that are deemed unnecessary or infeasible to include in the schedule.

    \item \textbf{Advance Timestep:} The agent advances the simulation a timestep, allowing the environment to update the state of operations and resources. This action is used when no immediate scheduling decisions can be made or when the agent determines that advancing time is the optimal strategy.
\end{itemize}

Each action is represented as a discrete choice, and the agent's policy maps the current state of the environment to one of these actions. 
The design of this action space ensures that the agent has the flexibility to make scheduling decisions while adhering to the constraints of the problem.

\section{Results}
\label{sse:results}

\subsection{Implemented Agent}
The implemented solution features a graph-based DQN agent designed to operate on the nodes of the train operation graph. 
At each decision step, the DQN agent is provided with a node from the graph and outputs one of the three possible actions: \textit{add node}, \textit{discard node}, or \textit{advance timestep}. 
If no node is deemed feasible at a given time, the environment automatically advances the simulation until the next decision point is reached. 
This ensures that the agent can continue making decisions without encountering unnecessary interruptions.

The full graph, which serves as the input to the agent, is processed by a GNN. 
The GNN computes embeddings for both nodes and edges, capturing the structural and relational information within the graph. 
These embeddings are then utilized by the DQN agent to output a solution, which attempts to effectively leverage the graph's topology and features to guide decision-making.

\subsection{Reward Function and Limitations}
The current reward function is designed to encourage the agent to compute feasible solutions efficiently. 
Specifically, the reward structure penalizes the agent for each decision it makes, incentivizing it to arrive at a solution in as few steps as possible. Additionally, the agent is rewarded for producing a feasible solution that adheres to the constraints of the Train Dispatching Problem.

However, a significant limitation of the current implementation is the absence of penalties for encountering deadlocks. 
Deadlocks, which occur when no further progress can be made without violating constraints, are critical to avoid in train dispatching scenarios. 
The lack of explicit penalties for deadlocks seems to lead the agent to overlook strategies that proactively prevent such situations. 
Future iterations of the reward function should address this limitation by incorporating penalties for deadlocks, ensuring that the agent learns to avoid gridlock.


\section{Conclusion and Future Work}
\label{sse:conclusion}

\subsection{Future Work}
\label{sss:future_work}
This work opens several avenues for future research and development:

\subsubsection{Improved Action Space Design}
The current action space, while functional, may not fully capture the complexities and nuances of the Train Dispatching Problem. 
Future work could explore more expressive and flexible action representations, such as hierarchical or multi-step actions, to better align with real-world dispatching scenarios. 
Additionally, incorporating domain-specific heuristics or constraints directly into the action space could improve the efficiency and feasibility of the learned policies.

\subsubsection{Graph Reinforcement Learning Algorithm Development}
The integration of GNNs with RL has shown promise in handling graph-structured data. 
However, the development of specialized graph RL algorithms tailored to the unique characteristics of train dispatching remains an open challenge. 
Future research could focus on designing algorithms that leverage the structural properties of train operation and resource conflict graphs, enabling more efficient learning and better generalization across different network configurations.

\subsubsection{Extensions to Other Applications}
While this work focuses on train dispatching, the proposed framework has the potential to be adapted to other domains involving complex scheduling and resource allocation problems. 
Examples include air traffic management, urban transit systems, and supply chain logistics. 
Extending the framework to these applications would require domain-specific modifications to the state and action spaces, as well as the underlying graph representations, but could significantly broaden the impact and utility of the approach.

\subsubsection{Action Space for Constraint Reduction}
The current action space focuses on selecting operations or advancing the simulation timestep. 
However, an alternative approach is to redefine the action space to directly influence the constraint or problem space, enabling the agent to assist in solving the Train Dispatching Problem in conjunction with Mixed-Integer Programming (MIP) solvers.

In this modified action space, the agent's actions could be designed to:
\begin{itemize}
    \item \textbf{Fix Routing Decisions:} The agent selects specific routing decisions for trains, effectively reducing the number of feasible solutions that the MIP solver must consider. For example, the agent could decide which track segment a train should use, thereby eliminating alternative routes from the problem space.
    \item \textbf{Prioritize Subproblems:} The agent determines which subproblems or regions of the railway network should be prioritized for optimization. This allows the MIP solver to focus computational resources on the most critical areas.
\end{itemize}

This hybrid approach has the potential to significantly improve scalability and solution quality, particularly for large-scale railway networks with complex constraints. 
Future work could explore the integration of this action space with state-of-the-art MIP solvers.

\subsection{Conclusion}
\label{sss:conclusion}

This honors thesis introduces a proof-of-concept reinforcement learning framework for the Train Dispatching Problem alongside preliminary results, emphasizing the formulation and potential of graph-based representations and deep learning techniques. 
By framing the problem as a Markov Decision Process and leveraging Graph Neural Networks, this work provides a foundational approach to train dispatching in the DISPLIB format that is both scalable and extensible.

As a formulation-focused study, this thesis aims to inspire future research in applying reinforcement learning to complex scheduling and resource allocation problems. 
Future efforts should prioritize refining the action space, developing tailored graph reinforcement learning algorithms, exploring other applications, and integrating reinforcement learning with mixed integer programming to achieve scalable optimal solutions. 
These advancements will build upon the groundwork laid here, contributing to the broader understanding and development of intelligent transportation systems.


% \section{Appendix}

% \begin{figure}
%     \centering
%     \begin{tikzpicture}[
%         qnet/.style={draw, rounded corners, fill=blue!20, minimum width=1.2cm, minimum height=1.2cm},
%         module/.style={draw, rounded corners, fill=purple!20, minimum width=1.2cm, minimum height=1.2cm},
%         action/.style={draw, rounded corners, fill=orange!30, minimum width=1.2cm, minimum height=1.2cm},
%         state/.style={draw, rounded corners, fill=green!20, minimum width=1.2cm, minimum height=1.2cm},
%         arrow/.style={->, thick, >=Stealth},
%         every node/.style={font=\scriptsize}
%       ]
%         % Vertical lines
%         \draw[thick] (0.8,0) -- (0.8,2.4);
%         \draw[thick] (1.6,0) -- (1.6,2.4);
%         % Horizontal lines
%         \draw[thick] (0,0.8) -- (2.4,0.8);
%         \draw[thick] (0,1.6) -- (2.4,1.6);
        
%         % Xs and Os
%         \node at (0.4, 2) {X};
%         \node at (1.2, 2) {O};
%         \node at (2, 2) {};
%         \node at (0.4, 1.2) {O};
%         \node at (1.2, 1.2) {};
%         \node at (2, 1.2) {O};
%         \node at (0.4, 0.4) {X};
%         \node at (1.2, 0.4) {};
%         \node at (2, 0.4) {};

%         \node[state] (state) at (4,1.2) {
%             $\begin{bmatrix}
%             s_1 = 1 \\ s_2 = -1 \\ s_3 = 0 \\ s_4 = -1 \\ s_5 = 0 \\ s_6 = -1 \\ s_7 = 1 \\ s_8 = 0 \\ s_9 = 0
%             \end{bmatrix}$
%         };
%         \draw[arrow] (2.4,1.2) -- (state);

%         \begin{scope}[shift={(6.5, 2)}]
%             % Box and title
%             \node[draw, rounded corners, thick, fill=blue!20, fit={(0, -1.6) (1.6, 0)}, inner sep=0.4cm, label={[yshift=-0.4cm]above:Q-Network}] (box) {};
            
%             % Input layer
%             \foreach \i in {1, 2} {
%                 \node[circle, draw, fill=green!20, minimum size=0.3cm] (input\i) at (0, -\i*0.4) {};
%             }
%             \node at (0, -1.2) {$\vdots$};
%             \node[circle, draw, fill=green!20, minimum size=0.3cm] (input3) at (0, -1.6) {};
            
%             % Hidden layer
%             \foreach \i in {1, 2} {
%                 \node[circle, draw, fill=purple!20, minimum size=0.3cm] (hidden\i) at (0.8, -\i*0.4-0.2) {};
%             }
            
%             % Output layer
%             \node[circle, draw, fill=orange!30, minimum size=0.3cm] (output) at (1.6, -0.8) {};
            
%             % Connections
%             \foreach \i in {1, 2, 3} {
%                 \foreach \j in {1, 2} {
%                     \draw[->, thick] (input\i) -- (hidden\j);
%                 }
%             }
%             \foreach \i in {1, 2} {
%                 \draw[->, thick] (hidden\i) -- (output);
%             }
%         \end{scope}
%         \draw[arrow] (state) -- (box);

%         \node[action] (action) at (10, 1.2) {Action};

%         \draw[arrow] (box) -- (action);
        
%     \end{tikzpicture}
%     \caption{Illustration of a Q-Network processing a Tic-Tac-Toe board state.}
%     \label{fig:qnetwork_tictactoe}
% \end{figure}


% \begin{figure}[htbp]
%     \centering

%     % ANN
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%         \begin{tikzpicture}[scale=0.8,
%             input/.style={circle,draw=blue!50,fill=blue!20,thick,minimum size=1.2em},
%             hidden/.style={circle,draw=black!60,fill=black!10,thick,minimum size=1.2em},
%             output/.style={circle,draw=red!70,fill=red!20,thick,minimum size=1.2em}
%         ]
%             % Input Layer
%             \foreach \i in {1,...,3}
%                 \node[input] (I\i) at (0,-\i) {};

%             % Hidden Layer
%             \foreach \i in {1,...,4}
%                 \node[hidden] (H\i) at (2,-\i+0.5) {};

%             % Output Layer
%             \foreach \i in {1,...,2}
%                 \node[output] (O\i) at (4,-\i-0.5) {};

%             % Connections
%             \foreach \i in {1,...,3}
%                 \foreach \j in {1,...,4}
%                     \draw[->] (I\i) -- (H\j);

%             \foreach \i in {1,...,4}
%                 \foreach \j in {1,...,2}
%                     \draw[->] (H\i) -- (O\j);
%         \end{tikzpicture}
%         \caption{Fully Connected ANN}
%     \end{subfigure}
%     \hfill

%     % CNN
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%             \begin{tikzpicture}[
%                 image/.style={draw, rounded corners, fill=blue!20, minimum width=2.5cm, minimum height=2.5cm},
%                 filter/.style={draw, thick, fill=red!20, minimum width=1cm, minimum height=1cm},
%                 feature/.style={draw, thick, fill=green!20, minimum width=1cm, minimum height=2cm},
%                 ann/.style={draw, thick, fill=purple!20, circle, minimum size=1cm},
%                 arrow/.style={->, thick, >=Stealth},
%                 every node/.style={font=\small}
%             ]

%             % Nodes
%             \node[image] (input) at (0, 0) {\includegraphics[width=2.5cm]{example-image}};
%             \node[filter] (filter) at (0, -2.5) {};
%             \node[feature] (feature) at (0, -5) {};
%             \node[ann] (ann) at (0, -7.5) {NN};

%             % Labels
%             \node[right] at (input.east) {28x28 Image};
%             \node[right] at (filter.east) {3x3 Filter};
%             \node[right] at (feature.east) {Pooled Features};
%             \node[right] at (ann.east) {Neural Network};

%             % Arrows
%             \draw[arrow] (input) -- (filter);
%             \draw[arrow] (filter) -- (feature);
%             \draw[arrow] (feature) -- (ann);

%             \end{tikzpicture}
%             \caption{Convolutional Neural Network (CNN)}
%     \end{subfigure}
%     \hfill

%     % GNN
%     \begin{subfigure}[b]{0.3\textwidth}
%         \centering
%             \begin{tikzpicture}[
%                 node/.style={circle, draw, fill=orange!30, minimum size=1cm},
%                 message/.style={draw, thick, fill=blue!20, minimum width=1.5cm, minimum height=0.8cm, rounded corners},
%                 aggregate/.style={draw, thick, fill=green!20, minimum width=1.5cm, minimum height=0.8cm, rounded corners},
%                 arrow/.style={->, thick, >=Stealth},
%                 doublearrow/.style={<->, dotted, thick, >=Stealth},
%                 every node/.style={font=\small}
%             ]

%             % Nodes
%             \node[node] (A) at (0, 0) {A};
%             \node[node] (B) at (2, 1.5) {B};
%             \node[node] (C) at (2, -1.5) {C};
%             \node[node] (D) at (4, 0) {D};

%             % Edges
%             \draw[doublearrow] (A) -- (B);
%             \draw[doublearrow] (A) -- (C);
%             \draw[doublearrow] (B) -- (D);
%             \draw[doublearrow] (C) -- (D);

%             % Message Passing
%             \node[message] (msg) at (2, -3) {Message Passing};
%             \draw[arrow] (C) -- (msg);
%             \draw[arrow] (A) -- (msg);
%             \draw[arrow] (D) -- (msg);

%             % Aggregation
%             \node[aggregate] (agg) at (2, -4.5) {Aggregation};
%             \draw[arrow] (msg) -- (agg);
%             % Pooling
%             \node[aggregate] (pooling) at (2, -6) {Pooling};
%             \draw[arrow] (agg) -- (pooling);
            

%             % Output
%             \node[node, fill=purple!30] (output) at (2, -7.5) {NN};
%             \draw[arrow] (pooling) -- (output);
%             \end{tikzpicture}
%             \caption{Graph Neural Network (GNN) with Message Passing and Aggregation Steps}
%     \end{subfigure}

%     \caption{Simplified visual comparison of common neural network architectures}
% \end{figure}

% \begin{figure}
%     \begin{tikzpicture}[>=Stealth, node distance=2cm and 2cm]

%         % Styles
%         \tikzstyle{op} = [draw, minimum width=1.8cm, minimum height=1.2cm]
%         \tikzstyle{circleop} = [op, circle]
%         \tikzstyle{redarrow} = [->, red, thick]
%         \tikzstyle{blackarrow} = [->, thick]
        
%         % Train A
%         \node[op] (a0) {op 0\\$r = [\,]$};
%         \node[circleop, above right=of a0] (a1) {op 1\\$r = [\,]$};
%         \node[circleop, below right=of a0] (a2) {op 2\\$r = [\,]$};
%         \node[circleop, right=of a1] (a3) {op 3\\$r = 1$};
%         \node[op, right=of a3] (a4) {op 4\\$r = [\,]$};
        
%         % Arrows for Train A
%         \draw[blackarrow] (a0) -- (a1);
%         \draw[blackarrow] (a0) -- (a2);
%         \draw[blackarrow] (a1) -- (a3);
%         \draw[blackarrow] (a3) -- (a4);
        
%         % Red connections (Train A feedback or dependencies)
%         \draw[redarrow] (a1) -- (a2);
%         \draw[redarrow] (a2) -- (a1);
%         \draw[redarrow] (a2) -- (a3);
%         \draw[redarrow] (a2) -- (a2);
        
%         % Train B
%         \node[op, below left=4cm and 2cm of a2] (b0) {op 0\\$r = [\,]$};
%         \node[circleop, right=of b0] (b1) {op 1\\$r = 1$};
%         \node[op, right=of b1] (b2) {op 2\\$r = [\,]$};
        
%         % Arrows for Train B
%         \draw[blackarrow] (b0) -- (b1);
%         \draw[blackarrow] (b1) -- (b2);
        
%         % Labels
%         \node[above left=0.5cm of a0] {\textbf{Train A}};
%         \node[above left=0.5cm of b0] {\textbf{Train B}};
        
%     \end{tikzpicture}
    
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{tikzpicture}[
%       qnet/.style={draw, rounded corners, fill=blue!20, minimum width=3cm, minimum height=1.2cm},
%       module/.style={draw, rounded corners, fill=purple!20, minimum width=3cm, minimum height=1.2cm},
%       action/.style={draw, rounded corners, fill=orange!30, minimum width=3cm, minimum height=1.2cm},
%       state/.style={draw, rounded corners, fill=green!20, minimum width=3cm, minimum height=1.2cm},
%       arrow/.style={->, thick, >=Stealth},
%       every node/.style={font=\small}
%     ]
    
%     % Nodes
%     \node[state] (state) at (0, 2) {State $s_t$};
%     \node[qnet] (qnet) at (0, 0.5) {Q-Network Approximator};
%     \node[module] (explore) at (0, -1) {Exploration Module};
%     \node[action] (action) at (0, -2.5) {Action $a_t$};
    
%     % Arrows
%     \draw[arrow] (state) -- (qnet);
%     \draw[arrow] (qnet) -- (explore);
%     \draw[arrow] (explore) -- (action);
%     \draw[arrow] (action.east) to[bend right=30] node[right] {} ++(2,1) 
%         to[bend right=60] node[left] {$s_{t+1}$} (state.east);

%     \end{tikzpicture}
%     \caption{Simplified Deep Q-Network action selection and feedback loop.}
%     \label{fig:dqn_loop}
% \end{figure}

\bibliographystyle{IEEEtran}
\bibliography{references}  % Assuming your .bib file is named references.bib


\end{document}



