% GENERAL DEEP REINFORCEMENT LEARNING PAPERS
@article{drl:Mnih13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602}
}
@article{drl:silver2017mastering,
  author       = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  title        = {Mastering the game of Go without human knowledge},
  journal      = {Nature},
  volume       = {550},
  pages        = {354--},
  year         = {2017},
  month        = {oct},
  publisher    = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  url          = {http://dx.doi.org/10.1038/nature24270}
}
@article{drl:silver2017alphazero,
  author       = {David Silver and
                  Thomas Hubert and
                  Julian Schrittwieser and
                  Ioannis Antonoglou and
                  Matthew Lai and
                  Arthur Guez and
                  Marc Lanctot and
                  Laurent Sifre and
                  Dharshan Kumaran and
                  Thore Graepel and
                  Timothy P. Lillicrap and
                  Karen Simonyan and
                  Demis Hassabis},
  title        = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
                  Learning Algorithm},
  journal      = {CoRR},
  volume       = {abs/1712.01815},
  year         = {2017},
  url          = {http://arxiv.org/abs/1712.01815},
  eprinttype    = {arXiv},
  eprint       = {1712.01815},
  timestamp    = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{drl:rl_supply_chain,
author = {Benjamin Rolf and Ilya Jackson and Marcel Müller and Sebastian Lang and Tobias Reggelin and Dmitry Ivanov and},
title = {A review on reinforcement learning algorithms and applications in supply chain management},
journal = {International Journal of Production Research},
volume = {61},
number = {20},
pages = {7151--7179},
year = {2023},
publisher = {Taylor \& Francis},
doi = {10.1080/00207543.2022.2140221},
URL = { https://doi.org/10.1080/00207543.2022.2140221},
eprint = { https://doi.org/10.1080/00207543.2022.2140221}

}

@Article{drl:rl_games_industry,
AUTHOR = {Souchleris, Konstantinos and Sidiropoulos, George K. and Papakostas, George A.},
TITLE = {Reinforcement Learning in Game Industry—Review, Prospects and Challenges},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {2443},
URL = {https://www.mdpi.com/2076-3417/13/4/2443},
ISSN = {2076-3417},
ABSTRACT = {This article focuses on the recent advances in the field of reinforcement learning (RL) as well as the present state–of–the–art applications in games. First, we give a general panorama of RL while at the same time we underline the way that it has progressed to the current degree of application. Moreover, we conduct a keyword analysis of the literature on deep learning (DL) and reinforcement learning in order to analyze to what extent the scientific study is based on games such as ATARI, Chess, and Go. Finally, we explored a range of public data to create a unified framework and trends for the present and future of this sector (RL in games). Our work led us to conclude that deep RL accounted for roughly 25.1% of the DL literature, and a sizable amount of this literature focuses on RL applications in the game domain, indicating the road for newer and more sophisticated algorithms capable of outperforming human performance.},
DOI = {10.3390/app13042443}
}


@misc{drl:robotics_review,
      title={Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes}, 
      author={Chen Tang and Ben Abbatematteo and Jiaheng Hu and Rohan Chandra and Roberto Martín-Martín and Peter Stone},
      year={2024},
      eprint={2408.03539},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2408.03539}, 
}

% DEEP REINFORCEMENT LEARNING WITH GRAPH NEURAL NETWORKS
@misc{gnndrl:munikoti2022challengesopportunitiesdeepreinforcement,
      title={Challenges and Opportunities in Deep Reinforcement Learning with Graph Neural Networks: A Comprehensive review of Algorithms and Applications}, 
      author={Sai Munikoti and Deepesh Agarwal and Laya Das and Mahantesh Halappanavar and Balasubramaniam Natarajan},
      year={2022},
      eprint={2206.07922},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.07922}, 
}
@misc{gnndrl:munikoti2022gramergraphmetareinforcement,
      title={GraMeR: Graph Meta Reinforcement Learning for Multi-Objective Influence Maximization}, 
      author={Sai Munikoti and Balasubramaniam Natarajan and Mahantesh Halappanavar},
      year={2022},
      eprint={2205.14834},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14834}, 
}
@article{gnndrl:Devailly_2022,
   title={IG-RL: Inductive Graph Reinforcement Learning for Massive-Scale Traffic Signal Control},
   volume={23},
   ISSN={1558-0016},
   url={http://dx.doi.org/10.1109/TITS.2021.3070835},
   DOI={10.1109/tits.2021.3070835},
   number={7},
   journal={IEEE Transactions on Intelligent Transportation Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Devailly, Francois-Xavier and Larocque, Denis and Charlin, Laurent},
   year={2022},
   month=jul, pages={7496–7507} }


% TRAIN DISPATCHING PROBLEM
@article{train:traindrl,
title = {Solving the train dispatching problem via deep reinforcement learning},
journal = {Journal of Rail Transport Planning & Management},
volume = {26},
pages = {100394},
year = {2023},
issn = {2210-9706},
doi = {https://doi.org/10.1016/j.jrtpm.2023.100394},
url = {https://www.sciencedirect.com/science/article/pii/S2210970623000264},
author = {Valerio Agasucci and Giorgio Grani and Leonardo Lamorgese},
keywords = {Scheduling, Reinforcement learning, Optimization},
abstract = {Every day, railways experience disturbances and disruptions, both on the network and the fleet side, that affect the stability of rail traffic. Induced delays propagate through the network, which leads to a mismatch in demand and offer for goods and passengers, and, in turn, to a loss in service quality. In these cases, it is the duty of human traffic controllers, the so-called dispatchers, to do their best to minimize the impact on traffic. However, dispatchers inevitably have a limited depth of perception of the knock-on effect of their decisions, particularly how they affect areas of the network that are outside their direct control. In recent years, much work in Decision Science has been devoted to developing methods to solve the problem automatically and support the dispatchers in this challenging task. This paper investigates Machine Learning-based methods for tackling this problem, proposing two different Deep Q-Learning methods(Decentralized and Centralized). Numerical results show the superiority of these techniques respect to the classical linear Q-Learning based on matrices. Moreover the Centralized approach is compared with a MILP formulation showing interesting results. The experiments are inspired on data provided by a U.S. class 1 railroad.}
}

@article{train:Sch_licke_2025,
   title={Solving the Real-Time Train Dispatching Problem by Column Generation},
   ISSN={1526-5447},
   url={http://dx.doi.org/10.1287/trsc.2023.0215},
   DOI={10.1287/trsc.2023.0215},
   journal={Transportation Science},
   publisher={Institute for Operations Research and the Management Sciences (INFORMS)},
   author={Schälicke, Maik and Nachtigall, Karl},
   year={2025},
   month=feb }


@article{train:LAMORGESE2013559,
title = {The track formulation for the Train Dispatching problem},
journal = {Electronic Notes in Discrete Mathematics},
volume = {41},
pages = {559-566},
year = {2013},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2013.05.138},
url = {https://www.sciencedirect.com/science/article/pii/S1571065313001418},
author = {Leonardo Lamorgese and Carlo Mannino},
keywords = {Rescheduling, dispatching, disjunctive formulation},
abstract = {With few exceptions, train movements are still controlled by human operators, the dispatchers. They establish routes and precedence between trains in real-time in order to cope with normal operations but also to recover from deviations from the timetable, and minimize overall delays. Implicitly they tackle and solve repeatedly a hard optimization problem, the Train Dispatching Problem. We recently developed a decomposition approach which allowed us to solve real-life instances to optimality or near optimality in times acceptable for dispatchers. We present here some new ideas which appear to significantly reduce computational times while solving to optimality even large instances.}
}

@article{train:doi:10.1287/opre.2014.1327,
author = {Lamorgese, Leonardo and Mannino, Carlo},
title = {An Exact Decomposition Approach for the Real-Time Train Dispatching Problem},
journal = {Operations Research},
volume = {63},
number = {1},
pages = {48-64},
year = {2015},
doi = {10.1287/opre.2014.1327},

URL = { 
    
        https://doi.org/10.1287/opre.2014.1327
    
    

},
eprint = { 
    
        https://doi.org/10.1287/opre.2014.1327
    
    

}
,
    abstract = { Trains’ movements on a railway network are regulated by official timetables. Deviations and delays occur quite often in practice, demanding fast rescheduling and rerouting decisions in order to avoid conflicts and minimize overall delay. This is the real-time train dispatching problem. In contrast with the classic “holistic” approach, we show how to decompose the problem into smaller subproblems associated with the line and the stations. This decomposition is the basis for a master-slave solution algorithm, in which the master problem is associated with the line and the slave problem is associated with the stations. The two subproblems are modeled as mixed integer linear programs, with specific sets of variables and constraints. Similarly to the classical Benders’ decomposition approach, slave and master communicate through suitable feasibility cuts in the variables of the master. Extensive tests on real-life instances from single and double-track lines in Italy showed significant improvements over current dispatching performances. A decision support system based on this exact approach has been in operation in Norway since February 2014 and represents one of the first operative applications of mathematical optimization to train dispatching. }
}

@incollection{train:lamorgese2018train,
  author    = {Leonardo Cameron Lamorgese and Carlo Mannino and Dario Pacciarelli and Johanna T{\"o}rnquist Krasemann},
  title     = {Train Dispatching},
  booktitle = {Handbook of Optimization in the Railway Industry},
  series    = {International Series in Operations Research \& Management Science},
  volume    = {268},
  year      = {2018},
  publisher = {Springer},
  url       = {http://hdl.handle.net/11250/2581671},
  note      = {Accepted version},
}

@misc{train:DISPLIB2025,
  title        = {{DISPLIB: Train Dispatching Benchmark Library}},
  author       = {{Bjørnar Luteberget and Giorgio Sartor and Oddvar Kloster and Carlo Mannino}},
  url          = {https://displib.github.io/},
  note         = {DISPLIB 2025 Competition organized by SINTEF, announced at ODS 2024},
  year         = {2024},
  month        = {September},
  institution  = {SINTEF Digital},
  accessdate   = {2025-04-23}
}